Okay day3 ( because foundations took 2 days) and now we are into one of the most simple yet elegant classical ML algos KNN

Weâ€™ll move in three phases:

1. The _intuition_ of what KNN really means.
2. The _mathematical and visual sense_ of it.
3. The _practical example_ that shows it in action.

---

## ğŸ§  1. The Deep Intuition â€” What Is Learning by â€œNeighborsâ€?

---

### ğŸ“Scenario: A New Person in Town

Suppose youâ€™ve just moved into a new city.  
You donâ€™t know anyone â€” but you see a few people around and notice their habits.

You notice some people:

- go to the arcade a lot,
- play video games,
- rarely go hiking.

Others:

- go running every morning,
- spend weekends outdoors,
- rarely play games.

Now a **new person** moves into your apartment building.  
You donâ€™t know if theyâ€™re a â€œgamerâ€ or an â€œoutdoorsyâ€ person.

But you do know **how much time they spend playing games** and **how much time they spend outdoors**.

---

### ğŸ’¡What would you do?

Youâ€™d think:

> â€œHmm, this person spends 8 hours gaming a week and 2 hours outdoors.  
> I know a few people like that â€” and they were mostly gamers.  
> So Iâ€™ll guess this person is a gamer too.â€

Thatâ€™s the essence of **k-Nearest Neighbors**.

Itâ€™s a _learning method without learning rules_.

---

### âš™ï¸ Itâ€™s Not a Formula â€” Itâ€™s a Comparison

Unlike algorithms that build an equation (like linear regression or SVM),  
k-NN **doesnâ€™t learn a function** during training.

It just **remembers all the data points**.

When a new data point comes along, it compares it to all the known ones,  
finds the _k_ most similar examples, and predicts the majority class.

Thatâ€™s why we call it **lazy learning** â€”  
it postpones â€œlearningâ€ until it needs to make a prediction.

---

## ğŸ§­ 2. The Geometry of â€œNeighborsâ€

Letâ€™s visualize whatâ€™s really happening.

Imagine you have this dataset:

| Person | Game hours | Outdoor hours | Label    |
| ------ | ---------- | ------------- | -------- |
| A      | 9          | 1             | Gamer    |
| B      | 8          | 2             | Gamer    |
| C      | 1          | 10            | Outdoors |
| D      | 2          | 9             | Outdoors |

Now you meet **E**, who plays 7 hours of games and spends 3 hours outdoors.  
You want to predict Eâ€™s label.

---

### Step 1: Plot It Mentally

If we plot â€œgame hoursâ€ on the x-axis and â€œoutdoor hoursâ€ on the y-axis,  
then:

- Gamers cluster on the **bottom-right** (lots of games, few outdoor hours),
- Outdoorsy people cluster on the **top-left** (few games, many outdoor hours).

E would land near the â€œgamerâ€ group.

So, intuitively, **E is probably a gamer**.

---

### Step 2: Measure Closeness (Distance)

We formalize â€œsimilarityâ€ as **distance** between points.

For E(7, 3):

- Distance to A(9,1)  
   = âˆš((9âˆ’7)Â² + (1âˆ’3)Â²) = âˆš(4 + 4) = âˆš8 â‰ˆ 2.83
- Distance to B(8,2)  
   = âˆš((8âˆ’7)Â² + (2âˆ’3)Â²) = âˆš(1 + 1) = âˆš2 â‰ˆ 1.41
- Distance to C(1,10)  
   = âˆš((1âˆ’7)Â² + (10âˆ’3)Â²) = âˆš(36 + 49) = âˆš85 â‰ˆ 9.22
- Distance to D(2,9)  
   = âˆš((2âˆ’7)Â² + (9âˆ’3)Â²) = âˆš(25 + 36) = âˆš61 â‰ˆ 7.81

The smallest distances are to **B and A**, both labeled _Gamer_.

---

### Step 3: Voting

Letâ€™s say _k = 3_ (we take the 3 nearest neighbors).

The three closest are:

1. B (Gamer)
2. A (Gamer)
3. D (Outdoors)

So, two gamers and one outdoorsy â†’ the **majority vote is â€œGamer.â€**

---

### Step 4: Why It Works

It works because of a simple assumption:

> â€œSimilar inputs tend to have similar outputs.â€

If two people have similar habits, they likely have similar preferences.

This assumption â€” called the **continuity assumption** â€” is what all nonparametric algorithms rely on.

---

## ğŸ§® 3. Mathematical Intuition

You can think of KNN as defining **regions** in the feature space.

For every new point, we look at which _region_ (neighborhood) it falls into.

When you set (k = 1), the boundary between classes is very sharp â€” each training point â€œownsâ€ the space closest to it.

When you increase (k), the boundary becomes smoother and more robust to noise.

Thatâ€™s why choosing (k) is about balancing **bias and variance**:

- Small (k): low bias, high variance (sensitive to noise)
- Large (k): high bias, low variance (over-smoothing)

---

## ğŸ’» 4. Translating Intuition into Code

Letâ€™s now connect intuition to a simple program.

Hereâ€™s the dataset we imagined:

```python
from numpy import array
import operator

def createDataSet():
    group = array([
        [9, 1],
        [8, 2],
        [1, 10],
        [2, 9]
    ])
    labels = ['Gamer', 'Gamer', 'Outdoors', 'Outdoors']
    return group, labels
```

And hereâ€™s our classifier:

```python
def classify0(inX, dataSet, labels, k):
    dataSetSize = dataSet.shape[0]
    diffMat = inX - dataSet
    sqDiffMat = diffMat ** 2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances ** 0.5
    sortedDistIndices = distances.argsort()
    classCount = {}
    for i in range(k):
        voteLabel = labels[sortedDistIndices[i]]
        classCount[voteLabel] = classCount.get(voteLabel, 0) + 1
    sortedClassCount = sorted(classCount.items(),
                              key=operator.itemgetter(1),
                              reverse=True)
    return sortedClassCount[0][0]
```

Now classify our new friend E(7, 3):

```python
group, labels = createDataSet()
print(classify0(array([7, 3]), group, labels, 3))
```

Output:

```
Gamer
```

Just like our intuition.

---

## ğŸ” 5. Understanding Why KNN Is â€œLazyâ€

Notice that KNN didnâ€™t **train** anything.  
Thereâ€™s no equation or model to store. It just keeps the dataset in memory.

When asked to classify a new point, it:

1. Computes distances to all known points,
2. Finds the closest ones,
3. Votes.

Thatâ€™s why itâ€™s sometimes called a **memory-based** or **instance-based** learner.

---

## âš–ï¸ 6. Pros and Cons

**Advantages:**

- Simple, intuitive, no training phase.
- Works well when the decision boundary is irregular.
- Naturally handles multi-class problems.

**Disadvantages:**

- Slow for large datasets (needs to compare to every point).
- Sensitive to the scale of features (so normalization is vital).
- Doesnâ€™t handle irrelevant features well â€” each one affects distance.

---

Now let's code this

## Data Normalization: Making Features Comparable

One critical concept for k-NN is that all features need to be on comparable scales. Imagine predicting house prices using both square footage (1000-5000) and number of bedrooms (1-5). The square footage would dominate the distance calculation simply because its values are larger, not because it's more important.
go to preprocessing.py:
